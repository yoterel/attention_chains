<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Attention (as-discrete time Markov) Chains">
  <meta property="og:title" content="Attention (as-discrete time Markov) Chains" />
  <meta property="og:description" content="Attention (as-discrete time Markov) Chains" />
  <meta property="og:url" content="https://github.com/yoterel/attention_chains" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/banner.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Attention, Markov chains, DTMC">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Attention (as-discrete time Markov) Chains</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Attention <em>(as discrete-time Markov)</em> Chains</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yoterel.github.io/" target="_blank">Yotam Erel*</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://odunkel.github.io/" target="_blank">Olaf DÃ¼nkel*</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://rishabhdabral.github.io/" target="_blank">Rishabh Dabral</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://people.mpi-inf.mpg.de/~golyanik/" target="_blank">Vladislav
                  Golyanik</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank">Christian
                  Theobalt</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.tau.ac.il/~amberman/" target="_blank">Amit H. Bermano</a><sup>1</sup>
              </span>
            </div>
            <div>
              *Denotes equal contribution
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Tel Aviv University <sup>2</sup>MPI for Informatics</span>
              <!--<span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>-->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/AttentionChains_supp.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/yoterel/attention_chains_code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We introduce a new interpretation of the attention matrix as a discrete-time Markov chain.
              Our interpretation sheds light on common operations involving attention scores such as selection,
              summation, and averaging in a unified framework.
              It further extends them by considering indirect attention, propagated through the Markov chain, as opposed
              to previous studies that only model immediate effects.
              Our main observation is that tokens corresponding to semantically similar regions form a set of
              <em>metastable</em> states, where the attention clusters, while noisy attention scores tend to disperse.
              Metastable states and their prevalence can be easily computed through simple matrix multiplication and
              eigenanalysis, respectively.
              Using these lightweight tools, we demonstrate state-of-the-art zero-shot segmentation.
              Lastly, we define <em>TokenRank</em>&mdash;the steady state vector of the Markov chain, which measures
              global
              token importance.
              We demonstrate that using it brings improvements in unconditional image generation.
              We believe our framework offers a fresh view of how tokens are being attended in modern visual
              transformers.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->
  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">Toy Example</h2>
            <img src="static/images/intuition.png" alt="intuition" class="center-image" />
            <div class="level-set has-text-justified">
              <p>
                <b>Left:</b>
                Attention matrix <b>A</b> with sequence length 5.
                <b>Middle:</b>
                A DTMC with transition probabilities defined by matrix <b>A</b>, where only strong connections are
                shown.
                <b>Right (One-Hot):</b>
                To evaluate where <em>state-4</em> attends to, we can iterate using the <em>power method</em> once
                starting from
                a one-hot vector (n=0), which results in the row-select operation (n=1).
                However, this first-order approximation is insufficient since state-0 mostly transitions to
                <em>state-3</em> and, therefore, <em>state-4</em> indirectly attends state-3.
                This becomes evident as we iterate further (n=2).
                <b>Right (Uniform):</b>
                To compute a global token ranking, we can iterate starting from a uniform state (n=0), resulting in a
                per-column sum operation (n=1). This indicates <em>state-0</em> as most important because many states
                have a high probability of transitioning into <em>state-0</em>.
                However, <em>state-0</em> maps to <em>state-3</em> with high probability, and <em>state-3</em> maps to
                <em>state-4</em> with high probability.
                Therefore, the importance of <em>state-4</em> should be elevated.
                When considering the second bounce (n=2), more probability mass is directed into <em>state-3</em>, and
                with a sufficient number of iterations the steady state ranks <em>state-4</em> as the
                most
                important state globally, which aligns with the intuition above.
              </p>
              <br>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">Applications</h2>
            <div class="content has-text-justified">
              <p>
                To support the usefullness of our framework, we show improvement in various downstream tasks such as
                zero-shot segmentation, and unconditional image generation.
              </p>

            </div>
            <img src="static/images/zero_shot.png" alt="zero_shot" class="center-image" />
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">TokenRank</h2>
            <div class="level-set has-text-justified">
              <p>
                TokenRank is the unique steady-state vector of the attention matrix, which can be used for
                visualizations and global understanding of both incoming and outgoing attention.
                It can serve as a standard tool for visualizing self-attention.
              </p>
            </div>
            <img src="static/images/tokenrank.png" alt="tokenrank" class="center-image" />
          </div>
        </div>
      </div>
    </div>
  </section>


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a>.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
